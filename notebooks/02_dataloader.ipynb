{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "490ae122",
   "metadata": {},
   "source": [
    "## DataLoader Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a847124",
   "metadata": {},
   "source": [
    "This notebook prepares the Flickr30K dataset for training. \n",
    "\n",
    "It loads and preprocesses the dataset, expands the data so that each image is paired with all its associated captions, tokenizes the captions using a pretrained DistilBERT tokenizer, and applies standard image transformations. Finally, it wraps everything into a PyTorch Dataset and DataLoader to efficiently feed batches of images and tokenized captions into a model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28feb782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\anaconda3\\envs\\contrastive-learn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from ast import literal_eval\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7d8a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "DATA_PATH = \"./data/raw/\"\n",
    "IMG_PATH = DATA_PATH + \"Flickr30k_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f5e0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Observations: 29000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>sentids</th>\n",
       "      <th>split</th>\n",
       "      <th>filename</th>\n",
       "      <th>img_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"Two young guys with shaggy hair look at thei...</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "      <td>train</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"Several men in hard hats are operating a gia...</td>\n",
       "      <td>[5, 6, 7, 8, 9]</td>\n",
       "      <td>train</td>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"A child in a pink dress is climbing up a set...</td>\n",
       "      <td>[10, 11, 12, 13, 14]</td>\n",
       "      <td>train</td>\n",
       "      <td>1000268201.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"Someone in a blue shirt and hat is standing ...</td>\n",
       "      <td>[15, 16, 17, 18, 19]</td>\n",
       "      <td>train</td>\n",
       "      <td>1000344755.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"Two men, one in a gray shirt, one in a black...</td>\n",
       "      <td>[20, 21, 22, 23, 24]</td>\n",
       "      <td>train</td>\n",
       "      <td>1000366164.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 raw               sentids  \\\n",
       "0  [\"Two young guys with shaggy hair look at thei...       [0, 1, 2, 3, 4]   \n",
       "1  [\"Several men in hard hats are operating a gia...       [5, 6, 7, 8, 9]   \n",
       "2  [\"A child in a pink dress is climbing up a set...  [10, 11, 12, 13, 14]   \n",
       "3  [\"Someone in a blue shirt and hat is standing ...  [15, 16, 17, 18, 19]   \n",
       "4  [\"Two men, one in a gray shirt, one in a black...  [20, 21, 22, 23, 24]   \n",
       "\n",
       "   split        filename  img_id  \n",
       "0  train  1000092795.jpg       0  \n",
       "1  train    10002456.jpg       1  \n",
       "2  train  1000268201.jpg       2  \n",
       "3  train  1000344755.jpg       3  \n",
       "4  train  1000366164.jpg       4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Annotations\n",
    "df = pd.read_csv(DATA_PATH + \"flickr_annotations_30k.csv\")\n",
    "df = df[df[\"split\"]==\"train\"].reset_index(drop=True)\n",
    "print(\"Total Observations:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a863ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Observations after exploding captions: 145000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def expand_captions(df, list_col='raw', filename_col='filename'):\n",
    "    df[list_col] = df[list_col].apply(literal_eval)\n",
    "    df_expanded = df.explode(list_col).rename(columns={list_col: 'caption'}).reset_index(drop=True)\n",
    "    df_expanded = df_expanded[[filename_col, 'caption']]\n",
    "    return df_expanded\n",
    "\n",
    "\n",
    "df_exploded = expand_captions(df)\n",
    "print(\"Total Observations after exploding captions:\", len(df_exploded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "690320ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokenized caption: \n",
      "Real caption: Two young guys with shaggy hair look at their hands while hanging out in the yard. \n",
      "Token: {'input_ids': tensor([[49406,  1237,  1888,  1791,   593, 42662,  2225,  1012,   536,   911,\n",
      "          3500,  1519,  4850,   620,   530,   518,  4313,   269, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# We Initialize the tokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "MAX_LENGTH = 64\n",
    "tokens = tokenizer(df_exploded['caption'][0], padding='max_length', truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "print(f\"Example tokenized caption: \\nReal caption: {df_exploded['caption'][0]} \\nToken: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32d141f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define image transformations for preprocessing based on CLIP\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),        \n",
    "    transforms.ToTensor(),                 \n",
    "    transforms.Normalize(                  \n",
    "        mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "        std=[0.26862954, 0.26130258, 0.27577711])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679143d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a Pytorch Dataset that returns both the processed image tensor and the tokenized caption.\n",
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, df, img_root, tokenizer, transform=None, max_length=64):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_root = img_root\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = f\"{self.img_root}/{row.filename}\"\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            row.caption,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokens = {k: v.squeeze(0) for k, v in tokens.items()}\n",
    "\n",
    "        return image, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0e0fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the dataset and create a DataLoader\n",
    "dataset = Flickr30kDataset(df_exploded, IMG_PATH, tokenizer, transform, max_length=MAX_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
